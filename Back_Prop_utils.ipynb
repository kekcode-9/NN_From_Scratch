{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Contents of <b>caches</b>:</h2>\n",
    "<br/>\n",
    "caches is a vector of length L.<br/>\n",
    "caches[ l ] is the cache returned by the forward_prop function on its l<sup>th</sup> call.<br/>\n",
    "this cache contains a linear_cache and an activation_cache. i.e.,\n",
    "<br/><b>caches[ l ] = ( linear_cache, activation_cache )</b><br/>\n",
    "<b>linear_cache = ( A<sup>[ l - 1 ]</sup> , W<sup>[ l ]</sup> , b<sup>[ l ]</sup> )</b><br/>and<br/>\n",
    "<b>activation_cache = ( Z<sup>[ l ]</sup> )</b><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Single Step of Back Propagation:</h2>\n",
    "<br/>\n",
    "Function for implementing one step of back_prop:\n",
    "<ul><li> Takes as input <b>dA</b>, <b>cache</b> and <b>activation</b>.<ul><li><b>dA</b> is the post-activation gradient for the current layer l i.e., dA<sup>[ l ]</sup>.\n",
    "<li><b> cache</b> is  a tuple of two tuples viz. <b>linear_cache</b> for layer l and <b>activation_cache</b> for layer l.\n",
    "<li><b>activation</b> is the name of the activation function used for layer l.\n",
    "</ul>\n",
    "<li> The function calculates \n",
    "<ul>\n",
    "    <li><b>dZ<sup>[ L ]</sup></b> as <b>dZ = AL - Y</b>. This is for softmax.\n",
    "    <li><b>dW<sup>[ L ]</sup></b> as <b>dW = ( dZ . transpose( A<sup>[ L - 1 ]</sup> ) ) / m</b>. This is for softmax.\n",
    "<li><b>dZ<sup>[ l ]</sup></b> as <b>dZ = dA * g<sup>[ l ] '</sup>( Z<sup>[ l ]</sup> )</b>. Note, here dA is same as dA<sup>[ l ]</sup>. shape( dZ<sup>[ l ]</sup> ) = shape( Z<sup>[ l ]</sup> ). g<sup>[ l ] '</sup>( ) is the gradient of g<sup>[ l ]</sup> ( ) w.r.t. <br/>Z<sup>[ l ]</sup>. This is for relu.\n",
    "<li><b>dW<sup>[ l ]</sup></b> as <b>dW = ( dZ<sup>[ l ]</sup> . transpose( A<sup>[ l - 1 ]</sup> ) ) / m</b>. shape( dW<sup>[ l ]</sup> ) = shape( W<sup>[ l ]</sup> ). This is for relu.\n",
    "<li><b>db<sup>[ l ]</sup></b> as <b>db = np.sum( dZ<sup>[ l ]</sup>, axis = 1, keepdims = True )</b>. shape( db<sup>[ l ]</sup> ) = shape( b<sup>[ l ]</sup> ). This is same for both softmax and relu.\n",
    "<li><b>dA<sup>[ l - 1 ]</sup></b> as <b> dA_prev = transpose( W<sup>[ l ]</sup> ) . dZ<sup>[ l ]</sup></b>. shape( dA<sup>[ l - 1 ]</sup> ) = shape( A<sup>[ l - 1 ]</sup> ).\n",
    "</ul> \n",
    "<li>The function returns <b>dA_prev</b>, <b>dW</b> and <b>db</b>.\n",
    "</ul>\n",
    "<br/>\n",
    "For softmax, instead of doing <b>dZ = AL - Y</b> we can do the same thing we do with relu i.e.,<br/>\n",
    "<b>dZ = dA * g<sup>[ l ] '</sup>( Z<sup>[ l ]</sup> )</b><br/>\n",
    "Here dA = - ( Y / A<sup>[ L ]</sup> ) and g<sup>[ l ] '</sup>( Z<sup>[ l ]</sup> ) = A<sup>[ L ]</sup> ( 1 - A<sup>[ L ]</sup> ) = g<sup>[ L ]</sup>( Z<sup>[ L ]</sup> ) ( 1 - g<sup>[ L ]</sup>( Z<sup>[ L ]</sup> ) ).<br/><br/>\n",
    "For relu, g<sup>[ l ] '</sup>( Z<sup>[ l ]</sup> ) = 1 if z > 0 and 0 if z < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_single_step(dA, Y, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    Z = activation_cache\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    if(activation=='softmax'):\n",
    "        #calculate g(Z). (1 -g(Z))\n",
    "        t = np.exp(Z) #element wise exponent of Z\n",
    "        sum_t = np.sum(t, axis=0) #for each col. of t, find sum of all rows in that col\n",
    "        gofZ = t / sum_t #calculate the softmax activation\n",
    "        #gDashOfZ = np.multiply(gofZ, (1 - gofZ))\n",
    "        dZ = gofZ - Y\n",
    "    if(activation=='relu'):\n",
    "#        temp = Z > 0\n",
    "#        gDashOfZ = temp.astype(int)\n",
    "#        dZ = np.multiply(dA, gDashOfZ)\n",
    "        gDashOfZ = np.ones_like(Z)\n",
    "        alpha = 0.01\n",
    "        gDashOfZ[ Z<0 ] = alpha\n",
    "        dZ = np.multiply(dA, gDashOfZ)\n",
    "        \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building L Layers  of Back Propagation:</h2>\n",
    "<br/>\n",
    "Function for building the L-layer model(L steps of back_prop):\n",
    "<ul>\n",
    "<li> Takes as argument <b>AL</b>, <b>Y</b>, <b>caches</b> where\n",
    "<ul>\n",
    "<li><b>AL</b> is A<sup>[ L ]</sup> i.e., the activation matrix of the last layer and of shape ( n<sup>[ L ]</sup>, m ).\n",
    "<li><b>Y</b> is the ground truth label vector of shape ( 1, m ).\n",
    "<li><b>caches</b> is the L element vector as returned by the L layer forward-prop model function.\n",
    "</ul>\n",
    "<li> The function calculates\n",
    "<ul>\n",
    "<li><b>dA<sup>[ L ]</sup></b> as <b>dAL = - ( Y / AL )</b>. It calls the one step back_prop function with dA<sup>[ L ]</sup> and caches[ L - 1 ] and activation as arguments.\n",
    "<li>For each of the L-1 hidden layers( in order L-1, L-2, ..., 1 ) it calls one step back_prop function with arguments dA<sup>[ l + 1 ]</sup>, caches[ l ] and activation name as arguments.\n",
    "</ul>\n",
    "<li> When one step back prop is called for the layer l of the NN, it returns dA<sup>[ l - 1 ]</sup>, dW<sup>[ l ]</sup> and db<sup>[ l ]</sup>. These are stored in a dictionary called <b>grads</b> as grads[ \"dA\" + str( l - 1 ) ], grads[ \"dW\" + str( l ) ] and grads[ \"db\" = str( l ) ] respectively.\n",
    "<li> The function returns the dict <b>grads</b>.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_Layer_Back_Prop(AL, Y, caches):\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    grads = {}\n",
    "    dAL = - ( np.divide(Y, AL) )\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = back_prop_single_step(dAL, Y, current_cache, activation='softmax')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\"+str(l)], grads[\"dW\"+str(l+1)], grads[\"db\"+str(l+1)] = back_prop_single_step(grads[ \"dA\"+str(l+1) ], Y, current_cache, activation='relu')\n",
    "        \n",
    "    return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Parameter Update:</h2>\n",
    "<br/>\n",
    "Function for updating parameters:\n",
    "<ul>\n",
    "<li> Takes as arguments the dict <b>parameters</b>, the dict <b>grads</b> and the learning rate <b>alpha</b>.\n",
    "<li> For l in range 0, 1, ..., L-1 it updates the values of parameters[ \"dW\" + str( l + 1 ) ] and parameters[ \"db\" + str( l + 1 ) ].\n",
    "<li> Returns the updated dict <b>parameters</b>.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, alpha):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[ \"W\"+str(l+1) ] = parameters[ \"W\"+str(l+1) ] - ( alpha * grads[ \"dW\"+str(l+1) ])\n",
    "        parameters[ \"b\"+str(l+1) ] = parameters[ \"b\"+str(l+1) ] - ( alpha * grads[ \"db\"+str(l+1) ])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
