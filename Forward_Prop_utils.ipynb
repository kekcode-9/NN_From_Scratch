{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Utility Functions for Forward Propagation Through The L Layered Model.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2>First we will load the train and test data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initializing Parameters:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for initializing weights and biases:<br/>\n",
    "<ul><li> Takes as argument a list called <b>layer_dims</b> that contains #layers values where each value gives the no. of  units in the respective layers.<br/>\n",
    "<li> <b> L = len(layer_dims)</b> is the #layers in the neural net. We have to initialize the weight matrices and bias vectors for L-1 layers(all except the 0 layer). For a layer l the weight matrix is W<sup>[ l ]</sup>( key in the dict is Wl ) and the bias is b<sup>[ l ]</sup>( key in the dict is bl ).<br/>\n",
    "<li> Returns a dictionary called <b>parameters</b> with ( L - 1 )*2 key-value pairs with keys W1, W2,...,WL and b1,b2,...,bL. The value corresponding to the key Wi is the matrix of weights for layer i. <br/>parameters[ \"W\" + str( i ) ] is a matrix of shape ( n<sup>[ i ]</sup>, n<sup>[ i - 1 ]</sup> ) and parameters[ \"b\"+str(i) ] is a vector of shape ( n<sup>[ i ]</sup>, 1 ). Initialize the weights and biases as random values.</ul><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    L = len(layer_dims) #no. of layers in the network, including input and output layers\n",
    "    parameters = {} #initialize the dict that will be returned\n",
    "    for l in range(1, L):\n",
    "        parameters[ \"W\"+str(l) ] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[ \"b\"+str(l) ] = np.zeros((layer_dims[l],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of W1:  (5, 4) \n",
      "\n",
      "shape of b1:  (5, 1) \n",
      "\n",
      "shape of W2:  (6, 5) \n",
      "\n",
      "shape of b2:  (6, 1) \n",
      "\n",
      "shape of W3:  (4, 6) \n",
      "\n",
      "shape of b3:  (4, 1) \n",
      "\n",
      "shape of W4:  (3, 4) \n",
      "\n",
      "shape of b4:  (3, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [4, 5, 6, 4, 3]\n",
    "\n",
    "#initializing parameters\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "print(\"shape of W1: \",parameters[\"W1\"].shape,\"\\n\")\n",
    "print(\"shape of b1: \",parameters[\"b1\"].shape,\"\\n\")\n",
    "print(\"shape of W2: \",parameters[\"W2\"].shape,\"\\n\")\n",
    "print(\"shape of b2: \",parameters[\"b2\"].shape,\"\\n\")\n",
    "print(\"shape of W3: \",parameters[\"W3\"].shape,\"\\n\")\n",
    "print(\"shape of b3: \",parameters[\"b3\"].shape,\"\\n\")\n",
    "print(\"shape of W4: \",parameters[\"W4\"].shape,\"\\n\")\n",
    "print(\"shape of b4: \",parameters[\"b4\"].shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Single Step of Forward Propagation:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for one step of forward propagation:<br/>\n",
    "<ul><li> This  calculates Z<sup>[ l ]</sup> and  A<sup>[ l ]</sup> for a single layer l.<br/>\n",
    "<li> Takes <b>A_prev</b>, <b>W</b>, <b>b</b> and <b>activation</b> for arguments. n<sup>[ l ]</sup> is the number of units in the l<sup>th</sup> layer.<br/>\n",
    "<ul><li><b>A_prev</b> is the activation vector(in form of a np array) for previous layer( [ l - 1 ] ) or the input X if we are at layer one.(X is layer zero). <i>Shape of A_prev is ( n<sup>[ l - 1 ]</sup>, m ).</i><br/>\n",
    "<li><b>W</b> is the weight matrix(in form of a list) of the current layer ( [ l ] ).<i> Shape of W is ( n<sup>[ l ]</sup>, n<sup>[ l - 1 ]</sup> ).</i><br/>\n",
    "<li><b>b</b> is the bias vector(in form of a list) of the current layer ( [ l ] ). <i>shape of b is ( n<sup>[ l ]</sup>, 1 ).</i><br/>\n",
    "<li><b>activation</b> is a string holding the name of the activation function.</ul><br/>\n",
    "<li> The function first calculates <b>Z = W . A_prev + b</b> and <i>shape of Z is ( n<sup>[ l ]</sup>, m ).</i> This is same as Z<sup>[ l ]</sup> = W<sup>[ l ]</sup> . A<sup>[ l - 1 ]</sup> + b<sup>[ l ]</sup>.<br/>\n",
    "<li> Then it calculates the activation <b>A = g( Z )</b> where g() is the activation function. <i>Shape of A is same ass shape of Z</i><br/>\n",
    "<li> Store A_prev, W, b in a tupple called <b>linear_cache</b>. Store Z in a tupple called <b>activation_cache</b>. These two caches will be required during back propagation.<br/>\n",
    "<li> The function returns <b>A</b> and <b>cache</b>, which is a tuple containing linear_cache and activation_cache.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_Prop_Single_Step(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    if(activation=='relu'):\n",
    "#        greaterThanZero = Z > 0 #this will return True for those values>0 and False otherwise. This gives an array of boolean values\n",
    "#        greaterThanZero = greaterThanZero.astype(int) #converts the array to an array of integers either 1 or 0\n",
    "#        A = np.multiply(greaterThanZero, Z) #for values of Z >0, returns the value, otherwise returns 0\n",
    "        leakyZ = np.multiply(0.01, Z)\n",
    "        A = np.maximum(Z, leakyZ)\n",
    "    if(activation=='softmax'):\n",
    "        t = np.exp(Z) #element wise exponent of Z\n",
    "        sum_t = np.sum(t, axis=0) #for each col. of t, find sum of all rows in that col.\n",
    "        A = t / sum_t #calculate the softmax activation\n",
    "    \n",
    "    linear_cache = (A_prev, W, b)\n",
    "    activation_cache = (Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building L layers of Forward Propagation: </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for building the L-layer model (L steps of forward propagation):<br/>\n",
    "<ul><li> Takes as input <b>X</b>, <b>Y</b> and <b>parameters</b> which is the output of the initialization function. <br/><b>L = len(parameters) // 2</b><br/>\n",
    "<li>Layer 0 is the input layer X. Layer L is the final layer. There are a total of L layers( not counting the input layer ). Number of hiddden layers is L-1. So we have to call the fuction implementing forward propagation L times: L-1 times for the L-1 hidden layers and once for the final layer.<br/>\n",
    "<li> For each layer l in range( 1, L ) i.e., 1, ..., ( L - 1 ) call Forward_Prop_Single_Step( ) and pass <b>A<sup>[ l - 1 ]</sup></b>, <b>parameters[ \"Wl\" ]</b> and <b>parameters[ \"bl\" ]</b> along with the  <b>'relu' activation</b> function as arguments. These are the forward prop steps performed for the L-1 hidden layers. The forward prop step for the final layer will be called separately with <b>'softmax' activation</b> function.<br/>\n",
    "<li> At each call to the forward_prop function A<sup>[ l ]</sup> and a cache is returned. Pass A<sup>[ l ]</sup> to the ( l + 1 )<sup>th</sup> call to the forward prop function. Append cache to a list called caches. Return <b>A<sup>[ L ]</sup></b> and <b>caches</b>. Shape of A<sup>[ L ]</sup> is (Y_train.shape[0], m)</ul><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_Forward_Prop(X, Y, parameters):\n",
    "    L = len(parameters) // 2\n",
    "    A_prev = X\n",
    "    caches = [] #this is the list to be returned\n",
    "    for l in range(1, L): #for each of the hidden layers\n",
    "        A, cache = Forward_Prop_Single_Step(A_prev, parameters[ \"W\"+str(l) ], parameters[ \"b\"+str(l) ], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "        A_prev = A\n",
    "        \n",
    "    #for the final layer\n",
    "    AL, cache = Forward_Prop_Single_Step(A_prev, parameters[ \"W\"+str(L) ], parameters[ \"b\"+str(L) ], activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (Y.shape[0], Y.shape[1])) #make sure AL has the right shape\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    correct = 0\n",
    "    #max_in_each_col is a (1, m) matrix whose i'th element contains the greatest value in the i'th col of AL\n",
    "    max_in_each_col = np.amax(AL, axis=0)\n",
    "    #find the one hot encoding for AL\n",
    "    one_hot_AL = np.where(AL<max_in_each_col, 0, 1)\n",
    "    for i in range(m):\n",
    "        if(np.array_equal(one_hot_AL[:, i], Y[:, i])):\n",
    "            correct = correct + 1\n",
    "    acc = (correct / m) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating the cost:</h2>\n",
    "<br/>\n",
    "Cost function:<br/>\n",
    "<ul><li> Takes <b>A<sup>[ L ]</sup></b> and <b>Y</b> as input and calculates the <b>cost</b>. A<sup>[ L ]</sup> is the vector of probabilities.<br/>\n",
    "    <li>Cost for the i<sup>th</sup> example is sum of all elements of Y<sup>( i )</sup>* log( A<sup>[ L ] ( i )</sup> ), which is a column vector with one element for each class.\n",
    "<li>Cost of m examples is the sum of the cost for each example.\n",
    "<li> Returns the <b>cost</b> as output.</ul><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    #since the activation at the last layer is softmax\n",
    "    #AL will be a (c, m) vector where c is the number of classes. Y is of the same shape.\n",
    "    #np.multiply(Y, np.log(AL)) gives a (c, m) vector. The row-wise sum(axis=0) outputs a (1, m) vector,\n",
    "    #where the i'th element is the cost for the i'th example. The outermost sum finds the cost for all m examples.\n",
    "    single_example_cost = - np.sum(np.multiply(Y, np.log(AL)), axis=0)\n",
    "    cost = np.sum(single_example_cost) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Contents of <b>caches</b>:</h2>\n",
    "<br/>\n",
    "caches is a vector of length L.<br/>\n",
    "caches[ l ] is the cache returned by the forward_prop function on its l<sup>th</sup> call.<br/>\n",
    "this cache contains a linear_cache and an activation_cache. i.e.,\n",
    "<br/><b>caches[ l ] = ( linear_cache, activation_cache )</b><br/>\n",
    "<b>linear_cache = ( A<sup>[ l - 1 ]</sup> , W<sup>[ l ]</sup> , b<sup>[ l ]</sup> )</b><br/>and<br/>\n",
    "<b>activation_cache = ( Z<sup>[ l ]</sup> )</b><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
